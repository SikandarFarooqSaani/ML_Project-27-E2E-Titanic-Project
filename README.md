# ğŸš¢ Titanic Survival Prediction  

![Titanic Logo](https://img.icons8.com/emoji/96/ship.png)  

---

## ğŸ“Œ Project Overview  
This is an **End-to-End Machine Learning project** predicting whether a passenger would survive the **Titanic disaster** ğŸŒŠ.  
The project includes **data preprocessing, model building, hyperparameter tuning, bagging, voting ensembles**, and finally **deployment on Streamlit**.  

ğŸ‘‰ **Live Demo:** [Streamlit App](https://lyzzxl47b7r4hvlhu79x27.streamlit.app/)  
ğŸ“‚ **GitHub Repo:** [Titanic Survival Prediction](https://github.com/SikandarFarooqSaani/Project-TitanicSurvivalPrediction)  

---

## ğŸ“‚ Dataset & Features  
The dataset is from the famous **Kaggle Titanic Competition**.  

We used **5 key features** for survival prediction:  
- Passenger Class ğŸŸï¸  
- Sex ğŸ‘¨â€ğŸ¦°ğŸ‘©â€ğŸ¦°  
- Age ğŸ‚  
- Fare ğŸ’°  
- Port of Embarkation âš“  

---

## âš™ï¸ Tech Stack & Libraries  
- **Python** ğŸ  
- **Pandas, NumPy** â†’ Data handling  
- **Scikit-learn** â†’ ML models, Bagging, RandomizedSearchCV  
- **Matplotlib, Seaborn** â†’ Visualization  
- **Joblib** â†’ Saving models (`.pkl`)  
- **Streamlit** â†’ Deployment  

---

## ğŸ› ï¸ Data Preprocessing  
- Dropped unnecessary columns ğŸ—‘ï¸  
- Filled missing values (NA)  
- Removed duplicates  
- Converted datatypes (this was a tricky part ğŸ˜…)  
- Applied **Label Encoding** to categorical columns  
- Saved the **cleaned dataset** for later use  

---

## ğŸš€ Models & Experiments  

### ğŸ”¹ Base Models  
- Logistic Regression â†’ **0.76**  
- Decision Tree â†’ **0.69**  
- KNN â†’ **0.70**  

ğŸ“Š Confusion Matrices (attached in repo):  
- Many misclassifications, especially False Negatives  
- KNN performed worst (likely due to **curse of dimensionality**)  

<img width="649" height="547" alt="27-1" src="https://github.com/user-attachments/assets/f2de4e36-8bfa-4a59-b562-468302ab2879" />

<img width="649" height="547" alt="27-2" src="https://github.com/user-attachments/assets/608610bc-1269-45e4-9271-c2b4e048c15c" />
<img width="649" height="547" alt="27-3" src="https://github.com/user-attachments/assets/3fae3ece-c3fc-4ca8-a845-1765eed34f1f" />




---

### ğŸ”¹ Hyperparameter Tuning (RandomizedSearchCV)  
- Logistic Regression â†’ **0.78** (best score)  
- Decision Tree â†’ **0.78**  
- KNN â†’ **0.74**  

Best parameters were displayed in the notebook.  
<img width="649" height="547" alt="27-4" src="https://github.com/user-attachments/assets/a9cb0003-5f89-47c9-bfc3-bc2dca493afc" />

---

### ğŸ”¹ Voting Classifier  
- Combined tuned Logistic, Decision Tree & KNN  
- Cross-Val Score â†’ **0.78**  
- Accuracy â†’ **0.77**  
- Confusion Matrix attached  
<img width="649" height="547" alt="27-5" src="https://github.com/user-attachments/assets/4fbdae26-41db-4d25-a011-d680bed93b3c" />

---

### ğŸ”¹ Bagging  
- Bagging with Decision Tree â†’ **0.79** âœ… (best balance of precision & recall)  
- Bagging with Logistic Regression â†’ **0.76**  
- Bagging with KNN â†’ **0.74**  

Confusion matrices for each are attached.  
<img width="649" height="547" alt="27-6" src="https://github.com/user-attachments/assets/d2acba10-8354-4f84-ac23-85447b278f61" />

<img width="649" height="547" alt="27-7" src="https://github.com/user-attachments/assets/c63bfb78-16e5-43ea-b9da-9df0e8d49f06" />

<img width="649" height="547" alt="27-8" src="https://github.com/user-attachments/assets/4150b89e-6482-4796-9efc-f059543ede2d" />



---

### ğŸ”¹ Advanced Models  
- Random Forest â†’ **0.73** (default), improved to **0.75** after tuning  
  - FN = 33, FP = 36  
- AdaBoost â†’ **0.8053** (best so far ğŸ‰)  
  - FN = 15, FP = 36  
  - After tuning â†’ **0.80 accuracy**  

---

## ğŸ“Š Results  

| Model                          | Accuracy | Notes |
|--------------------------------|----------|-------|
| Logistic Regression            | 0.76     | Baseline |
| Decision Tree                  | 0.69     | Overfit tendencies |
| KNN                            | 0.70     | Struggled with high dimensions |
| Voting Classifier (tuned)      | 0.77     | Balanced ensemble |
| Bagging (Decision Tree)        | **0.79** âœ… | Best recall/precision |
| Random Forest                  | 0.73â€“0.75 | Improved after tuning |
| AdaBoost                       | **0.80â€“0.8053** ğŸ† | Best overall |

---

## ğŸ’¡ Key Takeaways  
- Preprocessing was **the hardest part** of this dataset ğŸ’€  
- Logistic Regression held up surprisingly well âš¡  
- KNN struggled due to **curse of dimensionality**  
- Bagging with Decision Tree was **the most stable** option  
- **AdaBoost performed best overall** with **0.80+ accuracy** ğŸ¯  
- Learned a lot about **ensembles, hyperparameter tuning, and model saving**  

---

## ğŸ¯ Conclusion  
This project gave me **hands-on intuition of how ensembles improve predictions**.  
Itâ€™s not the perfect Kaggle solution, but as a learner, I feel Iâ€™ve **leveled up** ğŸš€.  

I will continue experimenting by adding more models and improving feature engineering. A new notebook with **Random Forest & AdaBoost experiments** is already included in the repo.  

---

âœ¨ *Survival isnâ€™t luck â€” itâ€™s feature engineering!* ğŸ˜…  
